{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to run classification on ESC50 or UrbanSound8K datasets. Input are spectrogram images transformed from the audio samples.  Look at DataPrep folder for functions to prepare the spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we load the images then order them into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from scipy import ndimage\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from params import *\n",
    "\n",
    "import model_single2 as m #import single-task learning CNN model\n",
    "\n",
    "#np.set_printoptions(threshold=np.nan)  #if want to see full output of big arrays in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_taken(elapsed):\n",
    "    \"\"\"To format time taken in hh:mm:ss. Use with time.monotic()\"\"\"\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "def get_subdirs(a_dir):\n",
    "    \"\"\" Returns a list of sub directory names in a_dir\"\"\" \n",
    "    return [name for name in os.listdir(a_dir)\n",
    "            if (os.path.isdir(os.path.join(a_dir, name)) and not (name.startswith('.')))]\n",
    "\n",
    "def esc50get_fold(string):\n",
    "    \"\"\"get fold no. from ESC-50 dataset using the filename. Labels #1-5\"\"\"\n",
    "    label_pos = string.index(\"-\")\n",
    "    return string[label_pos-1]\n",
    "\n",
    "def us8kget_fold(string):\n",
    "    \"\"\"use to grab fold no. of the UrbanSound8K dataset \n",
    "    e.g. filename: ../13230-0-0-1__fold3.tif\n",
    "    we want to get the number 3 after 'fold' which tells us the correct fold number\"\"\" \n",
    "    try:\n",
    "        found = re.search('__fold(.+?).tif', string).group(1)\n",
    "        return found\n",
    "    except AttributeError:\n",
    "        print(\"No matched string found.\")\n",
    "        return None\n",
    "\n",
    "def get_train_test(data,test_proportion,seed):\n",
    "    \"\"\"Splits data into arrays of test and training samples according to test_proportion\n",
    "    Can use train_test_split from sklearn.model_selection library for similar functionality \n",
    "    \"\"\"  \n",
    "    random.seed(seed)\n",
    "    test_samples = random.sample(data, int(len(data) * test_proportion)) #randomly pick out test samples\n",
    "    train_samples = list(set(data) - set(test_samples)) #the compliment will be used for training\n",
    "    return train_samples, test_samples\n",
    "\n",
    "def fetch_files_kfold(spec_dir,dataset):\n",
    "    \"\"\"Returns lists of full pathnames of datafiles and their respective (numerical) labels and folds\n",
    "    dataset: \"US8K\" or \"ESC50\"\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    folds = []\n",
    "    i = 0\n",
    "    class_folders = get_subdirs(spec_dir)\n",
    "    for folder in class_folders:\n",
    "        class_files = os.listdir(spec_dir + \"/\" + folder)\n",
    "        if dataset == \"US8K\":\n",
    "            fold = [int(us8kget_fold(fp)) for fp in class_files]\n",
    "        elif dataset == \"ESC50\":\n",
    "            fold = [int(esc50get_fold(fp)) for fp in class_files]\n",
    "        else:\n",
    "            raise ValueError(\"please only enter 'US8K' or 'ESC50'\")\n",
    "        class_files = [spec_dir + \"/\" + folder + \"/\" + fp for fp in class_files]\n",
    "        class_labels = [i] * len(class_files)\n",
    "        #print('Distribution of folds in class', i, ':', c.Counter(fold))\n",
    "        \n",
    "        folds.extend(fold)\n",
    "        filepaths.extend(class_files)\n",
    "        labels.extend(class_labels)\n",
    "        i += 1\n",
    "    return filepaths, labels, folds\n",
    "\n",
    "def fetch_files_holdout(spec_dir,hold_prop,seed):\n",
    "    \"\"\"Returns lists of full pathnames of datafiles and their respective (numerical) labels, split to training and test sets\n",
    "    hold_prop: proportion between 0-1 to hold out as test set in each class\n",
    "    seed: random seed for pseudorandom number generator. Use same seed to maintain same test set.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    train_filepaths = []\n",
    "    test_filepaths = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "    i = 0\n",
    "    class_folders = get_subdirs(spec_dir)\n",
    "    \n",
    "    for folder in class_folders:\n",
    "        class_files = os.listdir(spec_dir + \"/\" + folder)\n",
    "        class_files = [spec_dir + \"/\" + folder + \"/\" + fp for fp in class_files]\n",
    "        class_labels = [i] * len(class_files)\n",
    "\n",
    "        train_files, test_files, train_label, test_label = train_test_split(class_files, class_labels,\n",
    "                                                                              test_size=hold_prop, random_state=seed)\n",
    "        train_filepaths.extend(train_files)\n",
    "        test_filepaths.extend(test_files)\n",
    "        train_labels.extend(train_label)\n",
    "        test_labels.extend(test_label)\n",
    "        i += 1\n",
    "    return train_filepaths,test_filepaths,train_labels,test_labels\n",
    "\n",
    "def fetch_files(spec_dir):\n",
    "    \"\"\"Returns lists of full pathnames of datafiles and their respective (numerical) labels\"\"\"\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    class_folders = get_subdirs(spec_dir)\n",
    "    for folder in class_folders:\n",
    "        class_files = os.listdir(spec_dir + \"/\" + folder)\n",
    "        class_files = [spec_dir + \"/\" + folder + \"/\" + fp for fp in class_files]\n",
    "        class_labels = [i] * len(class_files)\n",
    "        \n",
    "        filepaths.extend(class_files)\n",
    "        labels.extend(class_labels)\n",
    "        i += 1\n",
    "    return filepaths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 4, 2: 4, 3: 4, 4: 4, 5: 4})\n"
     ]
    }
   ],
   "source": [
    "# firstly retrieve all the filenames, labels and folds\n",
    "import collections as c\n",
    "\n",
    "filepaths,labels,folds = fetch_files_kfold(STFT_dataset_path,dataset_name)\n",
    "print(c.Counter(folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(dataset, labels):\n",
    "    \"\"\"Randomizes order of elements in input arrays\"\"\"\n",
    "    import random\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:,:]\n",
    "    shuffled_labels = labels[permutation,:]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "def read_image(data):\n",
    "    \"\"\"For an array of filenames load each image. Returns a numpy array\"\"\"\n",
    "    images = np.array([np.array(ndimage.imread(fp).astype(float)) - 0.5 for fp in data])\n",
    "    return images\n",
    "    \n",
    "def sort_by_fold(folds,images,labels):\n",
    "    \"\"\"Sort images and labels in the order of folds\n",
    "    folds: the 'folds' output of fetch_files_kfold\n",
    "    * no longer have to sort by fold since the correct fold is picked during training time\n",
    "    \"\"\"\n",
    "    fold_order = np.argsort(folds, kind=\"mergesort\")\n",
    "    sorted_folds = np.array(folds)[fold_order]\n",
    "    sorted_images = images[fold_order] \n",
    "    sorted_labels = np.array(labels)[fold_order]\n",
    "    return sorted_images, sorted_labels, sorted_folds\n",
    "\n",
    "def reformat(dataset, labels, n_labels):\n",
    "    \"\"\"Reformats to appropriate shape for tensorflow covnet. Use 1-hot encoding for labels\"\"\"\n",
    "    dataset = dataset.reshape((-1, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)).astype(np.float32)\n",
    "    labels = (np.arange(n_labels) == labels[:,None]).astype(np.int32)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset (20, 513, 214, 1) (20, 2)\n"
     ]
    }
   ],
   "source": [
    "#read in images, and reformat to Tensorflow-friendly array shapes\n",
    "\n",
    "all_images = read_image(filepaths)\n",
    "all_labels = np.array(labels)\n",
    "folds = np.array(folds)\n",
    "images, labels = reformat(all_images, all_labels, N_LABELS)\n",
    "print('Dataset',images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions to save/load numpy arrays to/from file\n",
    "\n",
    "def save_sets(sets,name):\n",
    "    \"\"\"Writes the data array to .npy file. Can be loaded using load_set.\n",
    "    sets: arrays to be saved. can take a list\n",
    "    name: string to name the file. follow same order as in sets \n",
    "    \"\"\" \n",
    "    ind = 0\n",
    "    for x in sets:\n",
    "        np.save(save_path + '/{}.npy'.format(name[ind]), x)\n",
    "        ind += 1\n",
    "\n",
    "def load_set(sets):\n",
    "    \"\"\"Load existing data arrays from .npy files. Use if have preexisting data or when you don't to reshuffle the dataset\"\"\"\n",
    "    return np.load('{}.npy'.format(sets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now time to train and test the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = save_path + \"/filewriter/\"\n",
    "checkpoint_path = save_path + \"/checkpoint/\"\n",
    "\n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_path): os.mkdir(checkpoint_path)\n",
    "\n",
    "#********************************************************************\n",
    "\n",
    "# tf Graph input placeholders\n",
    "x = tf.placeholder(tf.float32, [batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "y = tf.placeholder(tf.int32, [None, N_LABELS])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "# Construct model\n",
    "pred = m.conv_net(x, m.weights, m.biases, keep_prob)\n",
    "\n",
    "#L2 regularization\n",
    "lossL2 = tf.add_n([tf.nn.l2_loss(val) for name,val in m.weights.items()]) * beta #L2 reg on all weight layers\n",
    "lossL2_onlyfull = tf.add_n([tf.nn.l2_loss(m.weights['wd1']),tf.nn.l2_loss(m.weights['out'])]) * beta #L2 reg on dense layers\n",
    "\n",
    "# Op for calculating the loss\n",
    "with tf.name_scope(\"cross_ent\"):\n",
    "    if l2reg:\n",
    "        if l2regfull:\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y) + lossL2_onlyfull)\n",
    "        else:\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y) + lossL2)\n",
    "    else:\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "# Train op\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(epsilon=epsilon).minimize(loss)\n",
    "\n",
    "# Add the loss to summary\n",
    "tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "# Predictions\n",
    "prob = tf.nn.softmax(pred)\n",
    "\n",
    "# Evaluation op: Accuracy of the model\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prob, 1), tf.argmax(y, 1))\n",
    "    accuracy = 100*tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Add the accuracy to the summary\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize an saver for store model checkpoints\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-07-17 14:55:09.951159 Open Tensorboard at --logdir C:/Users/Huz/Documents/python_scripts/Comparing_TF_representations/compare_TF_rep/Results/filewriter/\n",
      "*** Initializing fold #1 as test set ***\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Dst tensor is not initialized.\n\t [[Node: train/zeros_4 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [800,2] values: [0 0][0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'train/zeros_4', defined at:\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-e9999edf56b4>\", line 34, in <module>\n    optimizer = tf.train.AdamOptimizer(epsilon=epsilon).minimize(loss)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 298, in minimize\n    name=name)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 412, in apply_gradients\n    self._create_slots(var_list)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 119, in _create_slots\n    self._zeros_slot(v, \"m\", self._name)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 656, in _zeros_slot\n    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 121, in create_zeros_slot\n    val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1370, in zeros\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 169, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Dst tensor is not initialized.\n\t [[Node: train/zeros_4 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [800,2] values: [0 0][0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32mC:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    468\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    470\u001b[0m   \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[Node: train/zeros_4 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [800,2] values: [0 0][0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bdb5a1abe0f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[1;31m# Initialize all variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[1;31m# Add the model graph to TensorBoard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \"\"\"\n\u001b[0;32m-> 1588\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3830\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3831\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3832\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Dst tensor is not initialized.\n\t [[Node: train/zeros_4 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [800,2] values: [0 0][0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n\nCaused by op 'train/zeros_4', defined at:\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tornado\\stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-7-e9999edf56b4>\", line 34, in <module>\n    optimizer = tf.train.AdamOptimizer(epsilon=epsilon).minimize(loss)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 298, in minimize\n    name=name)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 412, in apply_gradients\n    self._create_slots(var_list)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\adam.py\", line 119, in _create_slots\n    self._zeros_slot(v, \"m\", self._name)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\", line 656, in _zeros_slot\n    named_slots[var] = slot_creator.create_zeros_slot(var, op_name)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\slot_creator.py\", line 121, in create_zeros_slot\n    val = array_ops.zeros(primary.get_shape().as_list(), dtype=dtype)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\", line 1370, in zeros\n    output = constant(zero, shape=shape, dtype=dtype, name=name)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\", line 169, in constant\n    attrs={\"value\": tensor_value, \"dtype\": dtype_value}, name=name).outputs[0]\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"C:\\Users\\Huz\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInternalError (see above for traceback): Dst tensor is not initialized.\n\t [[Node: train/zeros_4 = Const[dtype=DT_FLOAT, value=Tensor<type: float shape: [800,2] values: [0 0][0]...>, _device=\"/job:localhost/replica:0/task:0/gpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "max_acc = []\n",
    "max_epochs = []\n",
    "fold_list = [x+1 for x in range(max(folds))]\n",
    "\n",
    "total_runs = TOTAL_RUNS\n",
    "\n",
    "start_time_long = time.monotonic()\n",
    "text_file = open(save_path + \"/stft-double_v2.txt\", \"w\") #save training data\n",
    "print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(), filewriter_path))\n",
    "\n",
    "run = 0\n",
    "while run < total_runs:\n",
    "    for i in fold_list:\n",
    "\n",
    "        test_acc_list = []\n",
    "\n",
    "        train_set = images[folds!=i] #first get the right fold in order of fold_list\n",
    "        train_label = labels[folds!=i]\n",
    "        test_set = images[folds==i]\n",
    "        test_label = labels[folds==i]\n",
    "\n",
    "        text_file.write('*** Initializing fold #%u as test set ***\\n' % i)\n",
    "        print('*** Initializing fold #%u as test set ***' % i)\n",
    "\n",
    "        train_batches_per_epoch, train_extra = divmod(train_label.shape[0],batch_size)\n",
    "        test_batches_per_epoch, test_extra = divmod(test_label.shape[0],batch_size)\n",
    "\n",
    "        # Initialize the FileWriter\n",
    "        writer = tf.summary.FileWriter(filewriter_path + str(i))\n",
    "\n",
    "        with tf.Session() as session:\n",
    "\n",
    "            # Initialize all variables        \n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # Add the model graph to TensorBoard\n",
    "            writer.add_graph(session.graph)\n",
    "\n",
    "            print(\"{} Start training...\".format(datetime.now()))\n",
    "\n",
    "            start_time = time.monotonic()\n",
    "\n",
    "            for e in range(epochs):\n",
    "                print(\"{} Epoch number: {}\".format(datetime.now(), e+1))\n",
    "                text_file.write(\"{} Epoch number: {}\\n\".format(datetime.now(), e+1))\n",
    "                \n",
    "                #shuffle order every epoch\n",
    "                train_set, train_label = shuffle(train_set, train_label)\n",
    "                test_set, test_label = shuffle(test_set, test_label)\n",
    "\n",
    "                step = 1\n",
    "\n",
    "                while step < train_batches_per_epoch:\n",
    "                    #create training mini-batch here\n",
    "                    offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "                    batch_data = train_set[offset:(offset + batch_size), :, :, :]\n",
    "                    batch_labels = train_label[offset:(offset + batch_size),:]\n",
    "                    \n",
    "                    #train and backprop\n",
    "                    session.run(optimizer, feed_dict= {x:batch_data, y:batch_labels, keep_prob:dropout})\n",
    "                    \n",
    "                    #run merged_summary to display progress on Tensorboard\n",
    "                    if (step % display_step == 0):               \n",
    "                        s = session.run(merged_summary, feed_dict={x: batch_data, y: batch_labels, keep_prob: 1.})\n",
    "                        writer.add_summary(s, e*train_batches_per_epoch + step) \n",
    "                    step += 1\n",
    "\n",
    "                test_acc = 0.\n",
    "                test_count = 0\n",
    "                for i in range(test_batches_per_epoch):\n",
    "                    #prepare test mini-batch\n",
    "                    offset = (i * batch_size) % (test_label.shape[0] - batch_size)\n",
    "                    test_batch = test_set[offset:(offset + batch_size), :, :, :]\n",
    "                    label_batch = test_label[offset:(offset + batch_size),:]\n",
    "\n",
    "                    acc = session.run(accuracy, feed_dict={x: test_batch, y: label_batch, keep_prob: 1.})\n",
    "\n",
    "                    test_acc += acc*batch_size\n",
    "                    test_count += 1*batch_size\n",
    "                \n",
    "                #sometimes we get leftovers if batch_size is not a factor of the training/test set\n",
    "                if test_extra != 0:\n",
    "                    test_batch = test_set[-test_extra,:,:,:]\n",
    "                    label_batch = test_label[-test_extra:,:]\n",
    "                    acc = session.run([accuracy,prob], feed_dict={x: test_batch, y: label_batch, keep_prob: 1.})\n",
    "\n",
    "                    test_acc += acc*test_extra\n",
    "                    test_count += 1*test_extra\n",
    "                \n",
    "                #calculate total test accuracy\n",
    "                test_acc /= test_count \n",
    "                print(\"{} Test Accuracy = {:.4f}\".format(datetime.now(),test_acc))\n",
    "                text_file.write(\"{} Test Accuracy = {:.4f}\\n\".format(datetime.now(),test_acc))\n",
    "                test_acc_list.append(test_acc)\n",
    "\n",
    "                #save checkpoint of the model\n",
    "                if ((e+1) % checkpoint_epoch == 0):  \n",
    "                    checkpoint_name = os.path.join(checkpoint_path, dataset_name+'model_fold'+i+'_epoch'+str(e+1)+'.ckpt')\n",
    "                    save_path = saver.save(session, checkpoint_name)  \n",
    "                    print(\"{} Model checkpoint saved at {}\".format(datetime.now(), checkpoint_name))\n",
    "\n",
    "            # find the max test score and the epoch it belongs to        \n",
    "            max_acc.append(max(test_acc_list))\n",
    "            max_epoch = test_acc_list.index(max(test_acc_list))\n",
    "            max_epochs.append(max_epoch) \n",
    "\n",
    "            elapsed_time = time.monotonic() - start_time\n",
    "            text_file.write(\"--- Training time taken: {} ---\\n\".format(time_taken(elapsed_time)))\n",
    "            print(\"--- Training time taken:\",time_taken(elapsed_time),\"---\")\n",
    "            print(\"------------------------\")\n",
    "        \n",
    "        # return the max accuracies of each fold and their respective epochs\n",
    "        print(max_acc)\n",
    "        print(max_epochs)\n",
    "    run += 1\n",
    "\n",
    "elapsed_time_long = time.monotonic() - start_time_long\n",
    "print(\"*** All runs completed ***\")\n",
    "text_file.write(\"Total time taken:\")\n",
    "text_file.write(time_taken(elapsed_time_long))\n",
    "print(\"Total time taken:\",time_taken(elapsed_time_long))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
