{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to run classification on ESC50 or UrbanSound8K datasets. Input are spectrogram images transformed from the audio samples.  Look at DataPrep folder for functions to prepare the spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we load the images then order them into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from scipy import ndimage\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from params import *\n",
    "import spectreader\n",
    "\n",
    "import model_single2 as m #import single-task learning CNN model\n",
    "\n",
    "#np.set_printoptions(threshold=np.nan)  #if want to see full output of big arrays in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_taken(elapsed):\n",
    "    \"\"\"To format time taken in hh:mm:ss. Use with time.monotic()\"\"\"\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "def get_subdirs(a_dir):\n",
    "    \"\"\" Returns a list of sub directory names in a_dir\"\"\" \n",
    "    return [name for name in os.listdir(a_dir)\n",
    "            if (os.path.isdir(os.path.join(a_dir, name)) and not (name.startswith('.')))]\n",
    "\n",
    "def esc50get_fold(string):\n",
    "    \"\"\"get fold no. from ESC-50 dataset using the filename. Labels #1-5\"\"\"\n",
    "    label_pos = string.index(\"-\")\n",
    "    return string[label_pos-1]\n",
    "\n",
    "def us8kget_fold(string):\n",
    "    \"\"\"use to grab fold no. of the UrbanSound8K dataset \n",
    "    e.g. filename: ../13230-0-0-1__fold3.tif\n",
    "    we want to get the number 3 after 'fold' which tells us the correct fold number\"\"\" \n",
    "    try:\n",
    "        found = re.search('__fold(.+?).tif', string).group(1)\n",
    "        return found\n",
    "    except AttributeError:\n",
    "        print(\"No matched string found.\")\n",
    "        return None\n",
    "\n",
    "def get_train_test(data,test_proportion,seed):\n",
    "    \"\"\"Splits data into arrays of test and training samples according to test_proportion\n",
    "    Can use train_test_split from sklearn.model_selection library for similar functionality \n",
    "    \"\"\"  \n",
    "    random.seed(seed)\n",
    "    test_samples = random.sample(data, int(len(data) * test_proportion)) #randomly pick out test samples\n",
    "    train_samples = list(set(data) - set(test_samples)) #the compliment will be used for training\n",
    "    return train_samples, test_samples\n",
    "\n",
    "def fetch_files_kfold(spec_dir,dataset):\n",
    "    \"\"\"Returns lists of full pathnames of datafiles and their respective (numerical) labels and folds\n",
    "    dataset: \"US8K\" or \"ESC50\"\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    folds = []\n",
    "    i = 0\n",
    "    class_folders = get_subdirs(spec_dir)\n",
    "    for folder in class_folders:\n",
    "        class_files = os.listdir(spec_dir + \"/\" + folder)\n",
    "        if dataset == \"US8K\":\n",
    "            fold = [int(us8kget_fold(fp)) for fp in class_files]\n",
    "        elif dataset == \"ESC50\":\n",
    "            fold = [int(esc50get_fold(fp)) for fp in class_files]\n",
    "        else:\n",
    "            raise ValueError(\"please only enter 'US8K' or 'ESC50'\")\n",
    "        class_files = [spec_dir + \"/\" + folder + \"/\" + fp for fp in class_files]\n",
    "        class_labels = [i] * len(class_files)\n",
    "        #print('Distribution of folds in class', i, ':', c.Counter(fold))\n",
    "        \n",
    "        folds.extend(fold)\n",
    "        filepaths.extend(class_files)\n",
    "        labels.extend(class_labels)\n",
    "        i += 1\n",
    "    return filepaths, labels, folds\n",
    "\n",
    "def fetch_files_holdout(spec_dir,hold_prop,seed):\n",
    "    \"\"\"Returns lists of full pathnames of datafiles and their respective (numerical) labels, split to training and test sets\n",
    "    hold_prop: proportion between 0-1 to hold out as test set in each class\n",
    "    seed: random seed for pseudorandom number generator. Use same seed to maintain same test set.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    train_filepaths = []\n",
    "    test_filepaths = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "    i = 0\n",
    "    class_folders = get_subdirs(spec_dir)\n",
    "    \n",
    "    for folder in class_folders:\n",
    "        class_files = os.listdir(spec_dir + \"/\" + folder)\n",
    "        class_files = [spec_dir + \"/\" + folder + \"/\" + fp for fp in class_files]\n",
    "        class_labels = [i] * len(class_files)\n",
    "\n",
    "        train_files, test_files, train_label, test_label = train_test_split(class_files, class_labels,\n",
    "                                                                              test_size=hold_prop, random_state=seed)\n",
    "        train_filepaths.extend(train_files)\n",
    "        test_filepaths.extend(test_files)\n",
    "        train_labels.extend(train_label)\n",
    "        test_labels.extend(test_label)\n",
    "        i += 1\n",
    "    return train_filepaths,test_filepaths,train_labels,test_labels\n",
    "\n",
    "def fetch_files(spec_dir):\n",
    "    \"\"\"Returns lists of full pathnames of datafiles and their respective (numerical) labels\"\"\"\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    class_folders = get_subdirs(spec_dir)\n",
    "    for folder in class_folders:\n",
    "        class_files = os.listdir(spec_dir + \"/\" + folder)\n",
    "        class_files = [spec_dir + \"/\" + folder + \"/\" + fp for fp in class_files]\n",
    "        class_labels = [i] * len(class_files)\n",
    "        \n",
    "        filepaths.extend(class_files)\n",
    "        labels.extend(class_labels)\n",
    "        i += 1\n",
    "    return filepaths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# firstly retrieve all the filenames, labels and folds\n",
    "import collections as c\n",
    "\n",
    "filepaths,labels,folds = fetch_files_kfold(STFT_dataset_path,dataset_name)\n",
    "print(c.Counter(folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(dataset, labels):\n",
    "    \"\"\"Randomizes order of elements in input arrays\"\"\"\n",
    "    import random\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:,:]\n",
    "    shuffled_labels = labels[permutation,:]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "def read_image(data):\n",
    "    \"\"\"For an array of filenames load each image. Returns a numpy array\"\"\"\n",
    "    images = np.array([np.array(ndimage.imread(fp).astype(float)) - 0.5 for fp in data])\n",
    "    return images\n",
    "    \n",
    "def sort_by_fold(folds,images,labels):\n",
    "    \"\"\"Sort images and labels in the order of folds\n",
    "    folds: the 'folds' output of fetch_files_kfold\n",
    "    * no longer have to sort by fold since the correct fold is picked during training time\n",
    "    \"\"\"\n",
    "    fold_order = np.argsort(folds, kind=\"mergesort\")\n",
    "    sorted_folds = np.array(folds)[fold_order]\n",
    "    sorted_images = images[fold_order] \n",
    "    sorted_labels = np.array(labels)[fold_order]\n",
    "    return sorted_images, sorted_labels, sorted_folds\n",
    "\n",
    "def reformat(dataset, labels, n_labels):\n",
    "    \"\"\"Reformats to appropriate shape for tensorflow covnet. Use 1-hot encoding for labels\"\"\"\n",
    "    dataset = dataset.reshape((-1, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)).astype(np.float32)\n",
    "    labels = (np.arange(n_labels) == labels[:,None]).astype(np.int32)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in images, and reformat to Tensorflow-friendly array shapes\n",
    "\n",
    "all_images = read_image(filepaths)\n",
    "all_labels = np.array(labels)\n",
    "folds = np.array(folds)\n",
    "images, labels = reformat(all_images, all_labels, N_LABELS)\n",
    "print('Dataset',images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions to save/load numpy arrays to/from file\n",
    "\n",
    "def save_sets(sets,name):\n",
    "    \"\"\"Writes the data array to .npy file. Can be loaded using load_set.\n",
    "    sets: arrays to be saved. can take a list\n",
    "    name: string to name the file. follow same order as in sets \n",
    "    \"\"\" \n",
    "    ind = 0\n",
    "    for x in sets:\n",
    "        np.save(save_path + '/{}.npy'.format(name[ind]), x)\n",
    "        ind += 1\n",
    "\n",
    "def load_set(sets):\n",
    "    \"\"\"Load existing data arrays from .npy files. Use if have preexisting data or when you don't to reshuffle the dataset\"\"\"\n",
    "    return np.load('{}.npy'.format(sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# below params assuming freq bins as channels\n",
    "parameters={\n",
    "    'k_height' : NUM_CHANNELS, #k_height, \n",
    "    'k_numFrames' : IMAGE_WIDTH, #k_numFrames, \n",
    "    'k_inputChannels' : IMAGE_HEIGHT, #k_inputChannels, \n",
    "    'K_NUMCONVLAYERS' : K_NUMCONVLAYERS, \n",
    "    'L1_CHANNELS' : L1_CHANNELS, \n",
    "    'L2_CHANNELS' : L2_CHANNELS, \n",
    "    'FC_SIZE' : FC_SIZE, \n",
    "    'K_ConvRows' : K_ConvRows, \n",
    "    'K_ConvCols' : K_ConvCols, \n",
    "    'k_ConvStrideRows' : k_ConvStrideRows, \n",
    "    'k_ConvStrideCols' : k_ConvStrideCols, \n",
    "    'k_poolRows' : k_poolRows, \n",
    "    'k_poolStrideRows' : k_poolStrideRows, \n",
    "    'k_downsampledHeight' : k_downsampledHeight, \n",
    "    'k_downsampledWidth' : k_downsampledWidth,\n",
    "    'freqorientation' : FLAGS.freqorientation\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(\"/home/huzaifah/Documents/compareTF/DataPrep/stft_png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_THREADS = 4\n",
    "\n",
    "# Will train data samples equal to either\n",
    "#    nepochs * number of data records in the combined list of TFRecord files\n",
    "#  or\n",
    "#    k_nbatchsize * knbatches\n",
    "#  which ever is smaller \n",
    "#\n",
    "# Note: if you don't run for an integer number of epochs, some samples may be over/under represented\n",
    "\n",
    "def getImage(fname, nepochs) :\n",
    "    \"\"\" Reads data from the prepaired *list* files in fnames of TFRecords, does some preprocessing \n",
    "    params:\n",
    "    fnames - list of filenames to read data from\n",
    "    nepochs - An integer (optional). Just fed to tf.string_input_producer().  Reads through all data num_epochs times before generating an OutOfRange error. None means read forever.\n",
    "    \"\"\"\n",
    "\n",
    "    label, image = spectreader.getImage(fname, nepochs)\n",
    "    image = tf.reshape(image,[IMAGE_HEIGHT*IMAGE_WIDTH]) #same as np.flatten\n",
    "\n",
    "    # re-define label as a \"one-hot\" vector \n",
    "    # it will be [0,1] or [1,0] here. \n",
    "    # This approach can easily be extended to more classes.\n",
    "    label=tf.stack(tf.one_hot(label-1, N_LABELS))\n",
    "    \n",
    "    return label, image\n",
    "\n",
    "def get_datafiles(a_dir, foldnumber):\n",
    "    \"\"\" Returns a list of files in adir that start with the string in foldnumber \"\"\" \n",
    "    recordslist = []\n",
    "    for i in foldnumber:\n",
    "        recordslist.extend([a_dir + '/' + name for name in os.listdir(a_dir)\n",
    "                if name.startswith(i)])\n",
    "    return recordslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getImage ['/home/huzaifah/Documents/compareTF/DataPrep/stft_png/fold1-00001-of-00002', '/home/huzaifah/Documents/compareTF/DataPrep/stft_png/fold1-00000-of-00002', '/home/huzaifah/Documents/compareTF/DataPrep/stft_png/fold2-00001-of-00002', '/home/huzaifah/Documents/compareTF/DataPrep/stft_png/fold2-00000-of-00002', '/home/huzaifah/Documents/compareTF/DataPrep/stft_png/fold3-00001-of-00002', '/home/huzaifah/Documents/compareTF/DataPrep/stft_png/fold3-00000-of-00002', '/home/huzaifah/Documents/compareTF/DataPrep/stft_png/fold4-00000-of-00002', '/home/huzaifah/Documents/compareTF/DataPrep/stft_png/fold4-00001-of-00002']\n",
      "Tensor(\"one_hot_1:0\", shape=(2,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "target, data = getImage(get_datafiles(STFT_dataset_path, ['fold1','fold2','fold3','fold4']), k_nepochs)\n",
    "imageBatch, labelBatch = tf.train.shuffle_batch(\n",
    "    [data, target], batch_size=BATCH_SIZE,\n",
    "    num_threads=NUM_THREADS,\n",
    "    allow_smaller_final_batch=True, #want to finish an epoch even if datasize doesn't divide by batchsize\n",
    "    enqueue_many=False, #IMPORTANT to get right, default=False - \n",
    "    capacity=1000,  #1000,\n",
    "    min_after_dequeue=500) #500\n",
    "\n",
    "vtarget, vdata = getImage(get_datafiles(STFT_dataset_path, ['fold5']), k_nepochs)\n",
    "vimageBatch, vlabelBatch = tf.train.batch(\n",
    "    [vdata, vtarget], batch_size=BATCH_SIZE,\n",
    "    num_threads=NUM_THREADS,\n",
    "    allow_smaller_final_batch=True, #want to finish an epoch even if datasize doesn't divide by batchsize\n",
    "    enqueue_many=False, #IMPORTANT to get right, default=False - \n",
    "    capacity=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: create placeholders for features (X) and labels (Y)\n",
    "# each label is a one hot vector.\n",
    "# 'None' here allows us to fill the placeholders with different size batches (which we do with training and validation batches)\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None,IMAGE_HEIGHT*IMAGE_WIDTH], name= \"X\")\n",
    "x_image = tf.reshape(X, [-1,IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS]) \n",
    "\n",
    "# if FLAGS.freqorientation == \"height\" :\n",
    "#     x_image = tf.reshape(x, [-1,k_height,k_numFrames,k_inputChannels]) \n",
    "# else :\n",
    "#     print('set up reshaping for freqbins as channels')\n",
    "#     foo1 = tf.reshape(x, [-1,k_freqbins,k_numFrames,1]) #unflatten (could skip this step if it wasn't flattenned in the first place!)\n",
    "#     x_image = tf.transpose(foo1, perm=[0,3,2,1]) #moves freqbins from height to channel dimension\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None,N_LABELS], name= \"Y\")  #labeled classes, one-hot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now time to train and test the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = save_path + \"/filewriter/\"\n",
    "checkpoint_path = save_path + \"/checkpoint/\"\n",
    "\n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_path): os.mkdir(checkpoint_path)\n",
    "\n",
    "#********************************************************************\n",
    "\n",
    "# tf Graph input placeholders\n",
    "x = tf.placeholder(tf.float32, [BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "y = tf.placeholder(tf.int32, [None, N_LABELS])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "# Construct model\n",
    "pred = m.conv_net(x, m.weights, m.biases, keep_prob)\n",
    "\n",
    "#L2 regularization\n",
    "lossL2 = tf.add_n([tf.nn.l2_loss(val) for name,val in m.weights.items()]) * beta #L2 reg on all weight layers\n",
    "lossL2_onlyfull = tf.add_n([tf.nn.l2_loss(m.weights['wd1']),tf.nn.l2_loss(m.weights['out'])]) * beta #L2 reg on dense layers\n",
    "\n",
    "# Op for calculating the loss\n",
    "with tf.name_scope(\"cross_ent\"):\n",
    "    if l2reg:\n",
    "        if l2regfull:\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y) + lossL2_onlyfull)\n",
    "        else:\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y) + lossL2)\n",
    "    else:\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "# Train op\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(epsilon=epsilon).minimize(loss)\n",
    "\n",
    "# Add the loss to summary\n",
    "tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "# Predictions\n",
    "prob = tf.nn.softmax(pred)\n",
    "\n",
    "# Evaluation op: Accuracy of the model\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prob, 1), tf.argmax(y, 1))\n",
    "    accuracy = 100*tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Add the accuracy to the summary\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize an saver for store model checkpoints\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_acc = []\n",
    "max_epochs = []\n",
    "fold_list = [x+1 for x in range(max(folds))]\n",
    "\n",
    "total_runs = TOTAL_RUNS\n",
    "\n",
    "start_time_long = time.monotonic()\n",
    "text_file = open(save_path + \"/stft-double_v2.txt\", \"w\") #save training data\n",
    "print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(), filewriter_path))\n",
    "\n",
    "run = 0\n",
    "while run < total_runs:\n",
    "    for i in fold_list:\n",
    "\n",
    "        test_acc_list = []\n",
    "\n",
    "        train_set = images[folds!=i] #first get the right fold in order of fold_list\n",
    "        train_label = labels[folds!=i]\n",
    "        test_set = images[folds==i]\n",
    "        test_label = labels[folds==i]\n",
    "\n",
    "        text_file.write('*** Initializing fold #%u as test set ***\\n' % i)\n",
    "        print('*** Initializing fold #%u as test set ***' % i)\n",
    "\n",
    "        train_batches_per_epoch, train_extra = divmod(train_label.shape[0],BATCH_SIZE)\n",
    "        test_batches_per_epoch, test_extra = divmod(test_label.shape[0],BATCH_SIZE)\n",
    "\n",
    "        # Initialize the FileWriter\n",
    "        writer = tf.summary.FileWriter(filewriter_path + str(i))\n",
    "\n",
    "        with tf.Session() as session:\n",
    "\n",
    "            # Initialize all variables        \n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # Add the model graph to TensorBoard\n",
    "            writer.add_graph(session.graph)\n",
    "\n",
    "            print(\"{} Start training...\".format(datetime.now()))\n",
    "\n",
    "            start_time = time.monotonic()\n",
    "\n",
    "            for e in range(epochs):\n",
    "                print(\"{} Epoch number: {}\".format(datetime.now(), e+1))\n",
    "                text_file.write(\"{} Epoch number: {}\\n\".format(datetime.now(), e+1))\n",
    "                \n",
    "                #shuffle order every epoch\n",
    "                train_set, train_label = shuffle(train_set, train_label)\n",
    "                test_set, test_label = shuffle(test_set, test_label)\n",
    "\n",
    "                step = 1\n",
    "\n",
    "                while step < train_batches_per_epoch:\n",
    "                    #create training mini-batch here\n",
    "                    offset = (step * BATCH_SIZE) % (train_label.shape[0] - BATCH_SIZE)\n",
    "                    batch_data = train_set[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "                    batch_labels = train_label[offset:(offset + BATCH_SIZE),:]\n",
    "                    \n",
    "                    #train and backprop\n",
    "                    session.run(optimizer, feed_dict= {x:batch_data, y:batch_labels, keep_prob:dropout})\n",
    "                    \n",
    "                    #run merged_summary to display progress on Tensorboard\n",
    "                    if (step % display_step == 0):               \n",
    "                        s = session.run(merged_summary, feed_dict={x: batch_data, y: batch_labels, keep_prob: 1.})\n",
    "                        writer.add_summary(s, e*train_batches_per_epoch + step) \n",
    "                    step += 1\n",
    "\n",
    "                test_acc = 0.\n",
    "                test_count = 0\n",
    "                for i in range(test_batches_per_epoch):\n",
    "                    #prepare test mini-batch\n",
    "                    offset = (i * BATCH_SIZE) % (test_label.shape[0] - BATCH_SIZE)\n",
    "                    test_batch = test_set[offset:(offset + BATCH_SIZE), :, :, :]\n",
    "                    label_batch = test_label[offset:(offset + BATCH_SIZE),:]\n",
    "\n",
    "                    acc = session.run(accuracy, feed_dict={x: test_batch, y: label_batch, keep_prob: 1.})\n",
    "\n",
    "                    test_acc += acc*BATCH_SIZE\n",
    "                    test_count += 1*BATCH_SIZE\n",
    "                \n",
    "                #sometimes we get leftovers if batch_size is not a factor of the training/test set\n",
    "                if test_extra != 0:\n",
    "                    test_batch = test_set[-test_extra,:,:,:]\n",
    "                    label_batch = test_label[-test_extra:,:]\n",
    "                    acc = session.run([accuracy,prob], feed_dict={x: test_batch, y: label_batch, keep_prob: 1.})\n",
    "\n",
    "                    test_acc += acc*test_extra\n",
    "                    test_count += 1*test_extra\n",
    "                \n",
    "                #calculate total test accuracy\n",
    "                test_acc /= test_count \n",
    "                print(\"{} Test Accuracy = {:.4f}\".format(datetime.now(),test_acc))\n",
    "                text_file.write(\"{} Test Accuracy = {:.4f}\\n\".format(datetime.now(),test_acc))\n",
    "                test_acc_list.append(test_acc)\n",
    "\n",
    "                #save checkpoint of the model\n",
    "                if ((e+1) % checkpoint_epoch == 0):  \n",
    "                    checkpoint_name = os.path.join(checkpoint_path, dataset_name+'model_fold'+i+'_epoch'+str(e+1)+'.ckpt')\n",
    "                    save_path = saver.save(session, checkpoint_name)  \n",
    "                    print(\"{} Model checkpoint saved at {}\".format(datetime.now(), checkpoint_name))\n",
    "\n",
    "            # find the max test score and the epoch it belongs to        \n",
    "            max_acc.append(max(test_acc_list))\n",
    "            max_epoch = test_acc_list.index(max(test_acc_list))\n",
    "            max_epochs.append(max_epoch) \n",
    "\n",
    "            elapsed_time = time.monotonic() - start_time\n",
    "            text_file.write(\"--- Training time taken: {} ---\\n\".format(time_taken(elapsed_time)))\n",
    "            print(\"--- Training time taken:\",time_taken(elapsed_time),\"---\")\n",
    "            print(\"------------------------\")\n",
    "        \n",
    "        # return the max accuracies of each fold and their respective epochs\n",
    "        print(max_acc)\n",
    "        print(max_epochs)\n",
    "    run += 1\n",
    "\n",
    "elapsed_time_long = time.monotonic() - start_time_long\n",
    "print(\"*** All runs completed ***\")\n",
    "text_file.write(\"Total time taken:\")\n",
    "text_file.write(time_taken(elapsed_time_long))\n",
    "print(\"Total time taken:\",time_taken(elapsed_time_long))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
