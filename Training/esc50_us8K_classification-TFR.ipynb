{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to run classification on ESC50 or UrbanSound8K datasets. Input are spectrogram images transformed from the audio samples.  Look at DataPrep folder for functions to prepare the spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we load the images then order them into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from scipy import ndimage\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from params import *\n",
    "\n",
    "import model_single2 as m #import single-task learning CNN model\n",
    "\n",
    "#np.set_printoptions(threshold=np.nan)  #if want to see full output of big arrays in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_taken(elapsed):\n",
    "    \"\"\"To format time taken in hh:mm:ss. Use with time.monotic()\"\"\"\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions to save/load numpy arrays to/from file\n",
    "\n",
    "def save_sets(sets,name):\n",
    "    \"\"\"Writes the data array to .npy file. Can be loaded using load_set.\n",
    "    sets: arrays to be saved. can take a list\n",
    "    name: string to name the file. follow same order as in sets \n",
    "    \"\"\" \n",
    "    ind = 0\n",
    "    for x in sets:\n",
    "        np.save(save_path + '/{}.npy'.format(name[ind]), x)\n",
    "        ind += 1\n",
    "\n",
    "def load_set(sets):\n",
    "    \"\"\"Load existing data arrays from .npy files. Use if have preexisting data or when you don't to reshuffle the dataset\"\"\"\n",
    "    return np.load('{}.npy'.format(sets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import utils.spectreader as spectreader\n",
    "\n",
    "def getImage(fnames, nepochs=None) :\n",
    "    \"\"\" Reads data from the prepaired *list* of files in fnames of TFRecords, does some preprocessing \n",
    "    params:\n",
    "    fnames - list of filenames to read data from\n",
    "    nepochs - An integer (optional). Just fed to tf.string_input_producer().  Reads through all data num_epochs times before generating an OutOfRange error. None means read forever.\n",
    "    \"\"\"\n",
    "    label, image = spectreader.getImage(fnames, nepochs)\n",
    "\n",
    "    #same as np.flatten\n",
    "    # I can't seem to make shuffle batch work on images in their native shapes.\n",
    "    image=tf.reshape(image,[k_freqbins*k_numFrames])\n",
    "\n",
    "    # re-define label as a \"one-hot\" vector \n",
    "    # it will be [0,1] or [1,0] here. \n",
    "    # This approach can easily be extended to more classes.\n",
    "    label=tf.stack(tf.one_hot(label-1, N_LABELS))\n",
    "    print (\"getImage returning\")\n",
    "    return label, image\n",
    "\n",
    "\n",
    "\n",
    "def get_TFR_folds(a_dir, foldnumlist):\n",
    "    \"\"\" Returns a list of files names in a_dir that start with foldX where X is from the foldnumlist\"\"\"\n",
    "    lis = []\n",
    "    for num in foldnumlist : \n",
    "        lis.extend([a_dir + '/' + name for name in os.listdir(a_dir)\n",
    "            if name.startswith(\"fold\"+str(num))])\n",
    "    return lis\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getImage ['../DataPrep/stft_png/fold1-00000-of-00001', '../DataPrep/stft_png/fold2-00000-of-00001', '../DataPrep/stft_png/fold3-00000-of-00001', '../DataPrep/stft_png/fold4-00000-of-00001']\n",
      "TFRecordReader produced Tensor(\"ReaderNumRecordsProducedV2:0\", shape=(), dtype=int64) records\n",
      "getImage returning\n",
      "getImage ['../DataPrep/stft_png/fold5-00000-of-00001']\n",
      "TFRecordReader produced Tensor(\"ReaderNumRecordsProducedV2_1:0\", shape=(), dtype=int64) records\n",
      "getImage returning\n"
     ]
    }
   ],
   "source": [
    "datanumlist=[1,2,3,4]\n",
    "validatenumlist=[5]\n",
    "\n",
    "datafnames=get_TFR_folds(\"../DataPrep/stft_png\", datanumlist)\n",
    "target, data = getImage(datafnames, nepochs=1)\n",
    "\n",
    "validatefnames=get_TFR_folds(\"../DataPrep/stft_png\", validatenumlist)\n",
    "vtarget, vdata = getImage(validatefnames, nepochs=1)\n",
    "\n",
    "NUM_THREADS=2\n",
    "k_batchsize = batch_size\n",
    "k_vbatchsize = 2\n",
    "\n",
    "imageBatch, labelBatch = tf.train.shuffle_batch(\n",
    "    [data, target], batch_size=k_batchsize,\n",
    "    num_threads=NUM_THREADS,\n",
    "    allow_smaller_final_batch=True, #want to finish an eposh even if datasize doesn't divide by batchsize\n",
    "    enqueue_many=False, #IMPORTANT to get right, default=False - \n",
    "    capacity=1000,  #1000,\n",
    "    min_after_dequeue=500) #500\n",
    "\n",
    "vimageBatch, vlabelBatch = tf.train.batch(\n",
    "    [vdata, vtarget], batch_size=k_vbatchsize,\n",
    "    num_threads=NUM_THREADS,\n",
    "    allow_smaller_final_batch=False, #want to finish an eposh even if datasize doesn't divide by batchsize\n",
    "    enqueue_many=False, #IMPORTANT to get right, default=False - \n",
    "    capacity=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelBatch is Tensor(\"shuffle_batch:1\", shape=(?, 2), dtype=float32)\n",
      "Y_Batch is [[ 1.  0.]\n",
      " [ 0.  1.]]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    \n",
    "sess = tf.Session()\n",
    "sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "enqueue_threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "\n",
    "try:\n",
    "    if coord.should_stop():\n",
    "        print(\"coord should stop\")\n",
    "\n",
    "    print(\"labelBatch is \" + str(labelBatch))\n",
    "    X_batch, Y_batch = sess.run([imageBatch, labelBatch])\n",
    "    print(\"Y_Batch is \" + str(Y_batch))\n",
    "\n",
    "except (tf.errors.OutOfRangeError) as e:\n",
    "    coord.request_stop(e)\n",
    "\n",
    "finally :\n",
    "    coord.request_stop()\n",
    "    coord.join(enqueue_threads)    \n",
    "\n",
    "sess.close()\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getImage ['../DataPrep/stft_png/fold1-00000-of-00001', '../DataPrep/stft_png/fold2-00000-of-00001', '../DataPrep/stft_png/fold3-00000-of-00001', '../DataPrep/stft_png/fold5-00000-of-00001']\n",
      "TFRecordReader produced Tensor(\"ReaderNumRecordsProducedV2_2:0\", shape=(), dtype=int64) records\n",
      "getImage returning\n",
      "getImage ['../DataPrep/stft_png/fold4-00000-of-00001']\n",
      "TFRecordReader produced Tensor(\"ReaderNumRecordsProducedV2_3:0\", shape=(), dtype=int64) records\n",
      "getImage returning\n"
     ]
    }
   ],
   "source": [
    "datanumlist=[1,2,3,5]\n",
    "validatenumlist=[4]\n",
    "\n",
    "datafnames=get_TFR_folds(\"../DataPrep/stft_png\", datanumlist)\n",
    "target, data = getImage(datafnames, nepochs=1)\n",
    "\n",
    "validatefnames=get_TFR_folds(\"../DataPrep/stft_png\", validatenumlist)\n",
    "vtarget, vdata = getImage(validatefnames, nepochs=1)\n",
    "\n",
    "NUM_THREADS=2\n",
    "k_batchsize = batch_size\n",
    "k_vbatchsize = 2\n",
    "\n",
    "imageBatch, labelBatch = tf.train.shuffle_batch(\n",
    "    [data, target], batch_size=k_batchsize,\n",
    "    num_threads=NUM_THREADS,\n",
    "    allow_smaller_final_batch=True, #want to finish an eposh even if datasize doesn't divide by batchsize\n",
    "    enqueue_many=False, #IMPORTANT to get right, default=False - \n",
    "    capacity=1000,  #1000,\n",
    "    min_after_dequeue=500) #500\n",
    "\n",
    "vimageBatch, vlabelBatch = tf.train.batch(\n",
    "    [vdata, vtarget], batch_size=k_vbatchsize,\n",
    "    num_threads=NUM_THREADS,\n",
    "    allow_smaller_final_batch=False, #want to finish an eposh even if datasize doesn't divide by batchsize\n",
    "    enqueue_many=False, #IMPORTANT to get right, default=False - \n",
    "    capacity=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelBatch is Tensor(\"shuffle_batch_1:1\", shape=(?, 2), dtype=float32)\n",
      "Y_Batch is [[ 0.  1.]\n",
      " [ 1.  0.]]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.group(tf.global_variables_initializer(), tf.local_variables_initializer()))\n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "enqueue_threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "\n",
    "try:\n",
    "    if coord.should_stop():\n",
    "        print(\"coord should stop\")\n",
    "\n",
    "    print(\"labelBatch is \" + str(labelBatch))\n",
    "    X_batch, Y_batch = sess.run([imageBatch, labelBatch])\n",
    "    print(\"Y_Batch is \" + str(Y_batch))\n",
    "\n",
    "except (tf.errors.OutOfRangeError) as e:\n",
    "    coord.request_stop(e)\n",
    "\n",
    "finally :\n",
    "    coord.request_stop()\n",
    "    coord.join(enqueue_threads)    \n",
    "\n",
    "sess.close()\n",
    "    \n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now time to train and test the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = save_path + \"/filewriter/\"\n",
    "checkpoint_path = save_path + \"/checkpoint/\"\n",
    "\n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_path): os.mkdir(checkpoint_path)\n",
    "\n",
    "#********************************************************************\n",
    "\n",
    "# tf Graph input placeholders\n",
    "x = tf.placeholder(tf.float32, [batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "y = tf.placeholder(tf.int32, [None, N_LABELS])\n",
    "keep_prob = tf.placeholder(tf.float32, (), name=\"keepProb\") #dropout (keep probability)\n",
    "\n",
    "# Construct model\n",
    "pred = m.conv_net(x, m.weights, m.biases, keep_prob)\n",
    "\n",
    "#L2 regularization\n",
    "lossL2 = tf.add_n([tf.nn.l2_loss(val) for name,val in m.weights.items()]) * beta #L2 reg on all weight layers\n",
    "lossL2_onlyfull = tf.add_n([tf.nn.l2_loss(m.weights['wd1']),tf.nn.l2_loss(m.weights['out'])]) * beta #L2 reg on dense layers\n",
    "\n",
    "# Op for calculating the loss\n",
    "with tf.name_scope(\"cross_ent\"):\n",
    "    if l2reg:\n",
    "        if l2regfull:\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y) + lossL2_onlyfull)\n",
    "        else:\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y) + lossL2)\n",
    "    else:\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "# Train op\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(epsilon=epsilon).minimize(loss)\n",
    "\n",
    "# Add the loss to summary\n",
    "tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "# Predictions\n",
    "prob = tf.nn.softmax(pred)\n",
    "\n",
    "# Evaluation op: Accuracy of the model\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prob, 1), tf.argmax(y, 1))\n",
    "    accuracy = 100*tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Add the accuracy to the summary\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize an saver for store model checkpoints\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_acc = []\n",
    "max_epochs = []\n",
    "fold_list = [x+1 for x in range(max(folds))]\n",
    "print(\"Fold List is \" + str(fold_list))\n",
    "\n",
    "total_runs = TOTAL_RUNS\n",
    "\n",
    "start_time_long = time.monotonic()\n",
    "text_file = open(save_path + \"/stft-double_v2.txt\", \"w\") #save training data\n",
    "print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(), filewriter_path))\n",
    "\n",
    "run = 0\n",
    "while run < total_runs:\n",
    "    for i in fold_list:\n",
    "\n",
    "        test_acc_list = []\n",
    "        trainfolds=[value for value in fold_list if value != i]\n",
    "        testfolds=[i]\n",
    "        \n",
    "        ##train_set = images[folds!=i] #first get the right fold in order of fold_list\n",
    "        ##train_label = labels[folds!=i]\n",
    "        ##test_set = images[folds==i]\n",
    "        ##test_label = labels[folds==i]\n",
    "\n",
    "        \n",
    "\n",
    "        text_file.write('*** Initializing fold #%u as test set ***\\n' % i)\n",
    "        print('*** Initializing fold #%u as test set ***' % i)\n",
    "        \n",
    "        print(\"train_set \" + str([folds!=i]))\n",
    "\n",
    "\n",
    "        # Initialize the FileWriter\n",
    "        writer = tf.summary.FileWriter(filewriter_path + str(i))\n",
    "        \n",
    "\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            # Initialize all variables        \n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # Add the model graph to TensorBoard\n",
    "            writer.add_graph(sess.graph)\n",
    "\n",
    "            print(\"{} Start training...\".format(datetime.now()))\n",
    "\n",
    "            start_time = time.monotonic()\n",
    "\n",
    "            ##for e in range(epochs):\n",
    "            ##    print(\"{} Epoch number: {}\".format(datetime.now(), e+1))\n",
    "            ##    text_file.write(\"{} Epoch number: {}\\n\".format(datetime.now(), e+1))\n",
    "            ##    \n",
    "            ##    #shuffle order every epoch\n",
    "            ##    train_set, train_label = shuffle(train_set, train_label)\n",
    "            ##    test_set, test_label = shuffle(test_set, test_label)\n",
    "            ##\n",
    "            \n",
    "            train_set, train_label = sess.run([imageBatch, labelBatch], feed_dict={flist : trainfolds })\n",
    "            test_set, test_label = sess.run([vimageBatch, vlabelBatch], feed_dict={flist : testfolds }\n",
    "\n",
    "            e = 0\n",
    "            step = 1\n",
    "\n",
    "            while step < train_batches_per_epoch:\n",
    "                print(\"step = \" + str(step))\n",
    "                #create training mini-batch here\n",
    "                offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "                batch_data = train_set[offset:(offset + batch_size), :, :, :]\n",
    "                batch_labels = train_label[offset:(offset + batch_size),:]\n",
    "\n",
    "                #train and backprop\n",
    "                sess.run(optimizer, feed_dict= {x:batch_data, y:batch_labels, keep_prob:dropout})\n",
    "\n",
    "                #run merged_summary to display progress on Tensorboard\n",
    "                print(\"run summary\")\n",
    "                if (step % display_step == 0):               \n",
    "                    s = sess.run(merged_summary, feed_dict={x: batch_data, y: batch_labels, keep_prob: 1.})\n",
    "                    ##writer.add_summary(s, e*train_batches_per_epoch + step) \n",
    "                    writer.add_summary(s, step)\n",
    "                step += 1\n",
    "\n",
    "            print(\"now test\")\n",
    "            test_acc = 0.\n",
    "            test_count = 0\n",
    "            for bi in range(test_batches_per_epoch):\n",
    "                print(\"test step = \" + str(bi))\n",
    "                #prepare test mini-batch\n",
    "                offset = (bi * batch_size) % (test_label.shape[0] - batch_size)\n",
    "                test_batch = test_set[offset:(offset + batch_size), :, :, :]\n",
    "                label_batch = test_label[offset:(offset + batch_size),:]\n",
    "\n",
    "                acc = sess.run(accuracy, feed_dict={x: test_batch, y: label_batch, keep_prob: 1.})\n",
    "\n",
    "                test_acc += acc*batch_size\n",
    "                test_count += 1*batch_size\n",
    "\n",
    "            #sometimes we get leftovers if batch_size is not a factor of the training/test set\n",
    "            if test_extra != 0:\n",
    "                test_batch = test_set[-test_extra,:,:,:]\n",
    "                label_batch = test_label[-test_extra:,:]\n",
    "                acc = sess.run([accuracy,prob], feed_dict={x: test_batch, y: label_batch, keep_prob: 1.})\n",
    "             \n",
    "                test_acc += acc*test_extra\n",
    "                test_count += 1*test_extra\n",
    "\n",
    "            #calculate total test accuracy\n",
    "            test_acc /= test_count \n",
    "            print(\"{} Test Accuracy = {:.4f}\".format(datetime.now(),test_acc))\n",
    "            text_file.write(\"{} Test Accuracy = {:.4f}\\n\".format(datetime.now(),test_acc))\n",
    "            test_acc_list.append(test_acc)\n",
    "\n",
    "            #save checkpoint of the model\n",
    "            if ((e+1) % checkpoint_epoch == 0):  \n",
    "                checkpoint_name = os.path.join(checkpoint_path, dataset_name+'model_fold'+str(i)+'_epoch'+str(e+1)+'.ckpt')\n",
    "                save_path = saver.save(sess, checkpoint_name) \n",
    "                print(\"{} Model checkpoint saved at {}\".format(datetime.now(), checkpoint_name))\n",
    "\n",
    "        # find the max test score and the epoch it belongs to        \n",
    "        max_acc.append(max(test_acc_list))\n",
    "        max_epoch = test_acc_list.index(max(test_acc_list))\n",
    "        max_epochs.append(max_epoch) \n",
    "\n",
    "        elapsed_time = time.monotonic() - start_time\n",
    "        text_file.write(\"--- Training time taken: {} ---\\n\".format(time_taken(elapsed_time)))\n",
    "        print(\"--- Training time taken:\",time_taken(elapsed_time),\"---\")\n",
    "        print(\"------------------------\")\n",
    "        \n",
    "        # return the max accuracies of each fold and their respective epochs\n",
    "        print(max_acc)\n",
    "        print(max_epochs)\n",
    "    run += 1\n",
    "\n",
    "writer.close()\n",
    "elapsed_time_long = time.monotonic() - start_time_long\n",
    "print(\"*** All runs completed ***\")\n",
    "text_file.write(\"Total time taken:\")\n",
    "text_file.write(time_taken(elapsed_time_long))\n",
    "print(\"Total time taken:\",time_taken(elapsed_time_long))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tflow35]",
   "language": "python",
   "name": "conda-env-tflow35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
