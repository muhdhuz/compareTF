{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook to run classification on ESC50 or UrbanSound8K datasets. Input are spectrogram images transformed from the audio samples.  Look at DataPrep folder for functions to prepare the spectrograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we load the images then order them into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from scipy import ndimage\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from params import *\n",
    "\n",
    "import model_single2 as m #import single-task learning CNN model\n",
    "\n",
    "#np.set_printoptions(threshold=np.nan)  #if want to see full output of big arrays in Jupyter notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_taken(elapsed):\n",
    "    \"\"\"To format time taken in hh:mm:ss. Use with time.monotic()\"\"\"\n",
    "    m, s = divmod(elapsed, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    return \"%d:%02d:%02d\" % (h, m, s)\n",
    "\n",
    "def get_subdirs(a_dir):\n",
    "    \"\"\" Returns a list of sub directory names in a_dir\"\"\" \n",
    "    return [name for name in os.listdir(a_dir)\n",
    "            if (os.path.isdir(os.path.join(a_dir, name)) and not (name.startswith('.')))]\n",
    "\n",
    "def esc50get_fold(string):\n",
    "    \"\"\"get fold no. from ESC-50 dataset using the filename. Labels #1-5\"\"\"\n",
    "    label_pos = string.index(\"-\")\n",
    "    return string[label_pos-1]\n",
    "\n",
    "def us8kget_fold(string):\n",
    "    \"\"\"use to grab fold no. of the UrbanSound8K dataset \n",
    "    e.g. filename: ../13230-0-0-1__fold3.tif\n",
    "    we want to get the number 3 after 'fold' which tells us the correct fold number\"\"\" \n",
    "    try:\n",
    "        found = re.search('__fold(.+?).tif', string).group(1)\n",
    "        return found\n",
    "    except AttributeError:\n",
    "        print(\"No matched string found.\")\n",
    "        return None\n",
    "\n",
    "def get_train_test(data,test_proportion,seed):\n",
    "    \"\"\"Splits data into arrays of test and training samples according to test_proportion\n",
    "    Can use train_test_split from sklearn.model_selection library for similar functionality \n",
    "    \"\"\"  \n",
    "    random.seed(seed)\n",
    "    test_samples = random.sample(data, int(len(data) * test_proportion)) #randomly pick out test samples\n",
    "    train_samples = list(set(data) - set(test_samples)) #the compliment will be used for training\n",
    "    return train_samples, test_samples\n",
    "\n",
    "def fetch_files_kfold(spec_dir,dataset):\n",
    "    \"\"\"Returns lists of full pathnames of datafiles and their respective (numerical) labels and folds\n",
    "    dataset: \"US8K\" or \"ESC50\"\n",
    "    \"\"\"\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    folds = []\n",
    "    i = 0\n",
    "    class_folders = get_subdirs(spec_dir)\n",
    "    for folder in class_folders:\n",
    "        class_files = os.listdir(spec_dir + \"/\" + folder)\n",
    "        if dataset == \"US8K\":\n",
    "            fold = [int(us8kget_fold(fp)) for fp in class_files]\n",
    "        elif dataset == \"ESC50\":\n",
    "            fold = [int(esc50get_fold(fp)) for fp in class_files]\n",
    "        else:\n",
    "            raise ValueError(\"please only enter 'US8K' or 'ESC50'\")\n",
    "        class_files = [spec_dir + \"/\" + folder + \"/\" + fp for fp in class_files]\n",
    "        class_labels = [i] * len(class_files)\n",
    "        #print('Distribution of folds in class', i, ':', c.Counter(fold))\n",
    "        \n",
    "        folds.extend(fold)\n",
    "        filepaths.extend(class_files)\n",
    "        labels.extend(class_labels)\n",
    "        i += 1\n",
    "    return filepaths, labels, folds\n",
    "\n",
    "def fetch_files_holdout(spec_dir,hold_prop,seed):\n",
    "    \"\"\"Returns lists of full pathnames of datafiles and their respective (numerical) labels, split to training and test sets\n",
    "    hold_prop: proportion between 0-1 to hold out as test set in each class\n",
    "    seed: random seed for pseudorandom number generator. Use same seed to maintain same test set.\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    train_filepaths = []\n",
    "    test_filepaths = []\n",
    "    train_labels = []\n",
    "    test_labels = []\n",
    "    i = 0\n",
    "    class_folders = get_subdirs(spec_dir)\n",
    "    \n",
    "    for folder in class_folders:\n",
    "        class_files = os.listdir(spec_dir + \"/\" + folder)\n",
    "        class_files = [spec_dir + \"/\" + folder + \"/\" + fp for fp in class_files]\n",
    "        class_labels = [i] * len(class_files)\n",
    "\n",
    "        train_files, test_files, train_label, test_label = train_test_split(class_files, class_labels,\n",
    "                                                                              test_size=hold_prop, random_state=seed)\n",
    "        train_filepaths.extend(train_files)\n",
    "        test_filepaths.extend(test_files)\n",
    "        train_labels.extend(train_label)\n",
    "        test_labels.extend(test_label)\n",
    "        i += 1\n",
    "    return train_filepaths,test_filepaths,train_labels,test_labels\n",
    "\n",
    "def fetch_files(spec_dir):\n",
    "    \"\"\"Returns lists of full pathnames of datafiles and their respective (numerical) labels\"\"\"\n",
    "    filepaths = []\n",
    "    labels = []\n",
    "    i = 0\n",
    "    class_folders = get_subdirs(spec_dir)\n",
    "    for folder in class_folders:\n",
    "        class_files = os.listdir(spec_dir + \"/\" + folder)\n",
    "        class_files = [spec_dir + \"/\" + folder + \"/\" + fp for fp in class_files]\n",
    "        class_labels = [i] * len(class_files)\n",
    "        \n",
    "        filepaths.extend(class_files)\n",
    "        labels.extend(class_labels)\n",
    "        i += 1\n",
    "    return filepaths, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# firstly retrieve all the filenames, labels and folds\n",
    "import collections as c\n",
    "\n",
    "filepaths,labels,folds = fetch_files_kfold(STFT_dataset_path,dataset_name)\n",
    "print(c.Counter(folds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(dataset, labels):\n",
    "    \"\"\"Randomizes order of elements in input arrays\"\"\"\n",
    "    import random\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:,:]\n",
    "    shuffled_labels = labels[permutation,:]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "\n",
    "def read_image(data):\n",
    "    \"\"\"For an array of filenames load each image. Returns a numpy array\"\"\"\n",
    "    images = np.array([np.array(ndimage.imread(fp).astype(float)) - 0.5 for fp in data])\n",
    "    return images\n",
    "    \n",
    "def sort_by_fold(folds,images,labels):\n",
    "    \"\"\"Sort images and labels in the order of folds\n",
    "    folds: the 'folds' output of fetch_files_kfold\n",
    "    * no longer have to sort by fold since the correct fold is picked during training time\n",
    "    \"\"\"\n",
    "    fold_order = np.argsort(folds, kind=\"mergesort\")\n",
    "    sorted_folds = np.array(folds)[fold_order]\n",
    "    sorted_images = images[fold_order] \n",
    "    sorted_labels = np.array(labels)[fold_order]\n",
    "    return sorted_images, sorted_labels, sorted_folds\n",
    "\n",
    "def reformat(dataset, labels, n_labels):\n",
    "    \"\"\"Reformats to appropriate shape for tensorflow covnet. Use 1-hot encoding for labels\"\"\"\n",
    "    dataset = dataset.reshape((-1, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS)).astype(np.float32)\n",
    "    labels = (np.arange(n_labels) == labels[:,None]).astype(np.int32)\n",
    "    return dataset, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#read in images, and reformat to Tensorflow-friendly array shapes\n",
    "\n",
    "all_images = read_image(filepaths)\n",
    "all_labels = np.array(labels)\n",
    "folds = np.array(folds)\n",
    "images, labels = reformat(all_images, all_labels, N_LABELS)\n",
    "print('Dataset',images.shape, labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#functions to save/load numpy arrays to/from file\n",
    "\n",
    "def save_sets(sets,name):\n",
    "    \"\"\"Writes the data array to .npy file. Can be loaded using load_set.\n",
    "    sets: arrays to be saved. can take a list\n",
    "    name: string to name the file. follow same order as in sets \n",
    "    \"\"\" \n",
    "    ind = 0\n",
    "    for x in sets:\n",
    "        np.save(save_path + '/{}.npy'.format(name[ind]), x)\n",
    "        ind += 1\n",
    "\n",
    "def load_set(sets):\n",
    "    \"\"\"Load existing data arrays from .npy files. Use if have preexisting data or when you don't to reshuffle the dataset\"\"\"\n",
    "    return np.load('{}.npy'.format(sets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now time to train and test the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "filewriter_path = save_path + \"/filewriter/\"\n",
    "checkpoint_path = save_path + \"/checkpoint/\"\n",
    "\n",
    "# Create parent path if it doesn't exist\n",
    "if not os.path.isdir(checkpoint_path): os.mkdir(checkpoint_path)\n",
    "\n",
    "#********************************************************************\n",
    "\n",
    "# tf Graph input placeholders\n",
    "x = tf.placeholder(tf.float32, [batch_size, IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS])\n",
    "y = tf.placeholder(tf.int32, [None, N_LABELS])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)\n",
    "\n",
    "# Construct model\n",
    "pred = m.conv_net(x, m.weights, m.biases, keep_prob)\n",
    "\n",
    "#L2 regularization\n",
    "lossL2 = tf.add_n([tf.nn.l2_loss(val) for name,val in m.weights.items()]) * beta #L2 reg on all weight layers\n",
    "lossL2_onlyfull = tf.add_n([tf.nn.l2_loss(m.weights['wd1']),tf.nn.l2_loss(m.weights['out'])]) * beta #L2 reg on dense layers\n",
    "\n",
    "# Op for calculating the loss\n",
    "with tf.name_scope(\"cross_ent\"):\n",
    "    if l2reg:\n",
    "        if l2regfull:\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y) + lossL2_onlyfull)\n",
    "        else:\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y) + lossL2)\n",
    "    else:\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "\n",
    "# Train op\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(epsilon=epsilon).minimize(loss)\n",
    "\n",
    "# Add the loss to summary\n",
    "tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "# Predictions\n",
    "prob = tf.nn.softmax(pred)\n",
    "\n",
    "# Evaluation op: Accuracy of the model\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax(prob, 1), tf.argmax(y, 1))\n",
    "    accuracy = 100*tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Add the accuracy to the summary\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize an saver for store model checkpoints\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_acc = []\n",
    "max_epochs = []\n",
    "fold_list = [x+1 for x in range(max(folds))]\n",
    "\n",
    "total_runs = TOTAL_RUNS\n",
    "\n",
    "start_time_long = time.monotonic()\n",
    "text_file = open(save_path + \"/stft-double_v2.txt\", \"w\") #save training data\n",
    "print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(), filewriter_path))\n",
    "\n",
    "run = 0\n",
    "while run < total_runs:\n",
    "    for i in fold_list:\n",
    "\n",
    "        test_acc_list = []\n",
    "\n",
    "        train_set = images[folds!=i] #first get the right fold in order of fold_list\n",
    "        train_label = labels[folds!=i]\n",
    "        test_set = images[folds==i]\n",
    "        test_label = labels[folds==i]\n",
    "\n",
    "        text_file.write('*** Initializing fold #%u as test set ***\\n' % i)\n",
    "        print('*** Initializing fold #%u as test set ***' % i)\n",
    "\n",
    "        train_batches_per_epoch, train_extra = divmod(train_label.shape[0],batch_size)\n",
    "        test_batches_per_epoch, test_extra = divmod(test_label.shape[0],batch_size)\n",
    "\n",
    "        # Initialize the FileWriter\n",
    "        writer = tf.summary.FileWriter(filewriter_path + str(i))\n",
    "\n",
    "        with tf.Session() as session:\n",
    "\n",
    "            # Initialize all variables        \n",
    "            tf.global_variables_initializer().run()\n",
    "\n",
    "            # Add the model graph to TensorBoard\n",
    "            writer.add_graph(session.graph)\n",
    "\n",
    "            print(\"{} Start training...\".format(datetime.now()))\n",
    "\n",
    "            start_time = time.monotonic()\n",
    "\n",
    "            for e in range(epochs):\n",
    "                print(\"{} Epoch number: {}\".format(datetime.now(), e+1))\n",
    "                text_file.write(\"{} Epoch number: {}\\n\".format(datetime.now(), e+1))\n",
    "                \n",
    "                #shuffle order every epoch\n",
    "                train_set, train_label = shuffle(train_set, train_label)\n",
    "                test_set, test_label = shuffle(test_set, test_label)\n",
    "\n",
    "                step = 1\n",
    "\n",
    "                while step < train_batches_per_epoch:\n",
    "                    #create training mini-batch here\n",
    "                    offset = (step * batch_size) % (train_label.shape[0] - batch_size)\n",
    "                    batch_data = train_set[offset:(offset + batch_size), :, :, :]\n",
    "                    batch_labels = train_label[offset:(offset + batch_size),:]\n",
    "                    \n",
    "                    #train and backprop\n",
    "                    session.run(optimizer, feed_dict= {x:batch_data, y:batch_labels, keep_prob:dropout})\n",
    "                    \n",
    "                    #run merged_summary to display progress on Tensorboard\n",
    "                    if (step % display_step == 0):               \n",
    "                        s = session.run(merged_summary, feed_dict={x: batch_data, y: batch_labels, keep_prob: 1.})\n",
    "                        writer.add_summary(s, e*train_batches_per_epoch + step) \n",
    "                    step += 1\n",
    "\n",
    "                test_acc = 0.\n",
    "                test_count = 0\n",
    "                for i in range(test_batches_per_epoch):\n",
    "                    #prepare test mini-batch\n",
    "                    offset = (i * batch_size) % (test_label.shape[0] - batch_size)\n",
    "                    test_batch = test_set[offset:(offset + batch_size), :, :, :]\n",
    "                    label_batch = test_label[offset:(offset + batch_size),:]\n",
    "\n",
    "                    acc = session.run(accuracy, feed_dict={x: test_batch, y: label_batch, keep_prob: 1.})\n",
    "\n",
    "                    test_acc += acc*batch_size\n",
    "                    test_count += 1*batch_size\n",
    "                \n",
    "                #sometimes we get leftovers if batch_size is not a factor of the training/test set\n",
    "                if test_extra != 0:\n",
    "                    test_batch = test_set[-test_extra,:,:,:]\n",
    "                    label_batch = test_label[-test_extra:,:]\n",
    "                    acc = session.run([accuracy,prob], feed_dict={x: test_batch, y: label_batch, keep_prob: 1.})\n",
    "\n",
    "                    test_acc += acc*test_extra\n",
    "                    test_count += 1*test_extra\n",
    "                \n",
    "                #calculate total test accuracy\n",
    "                test_acc /= test_count \n",
    "                print(\"{} Test Accuracy = {:.4f}\".format(datetime.now(),test_acc))\n",
    "                text_file.write(\"{} Test Accuracy = {:.4f}\\n\".format(datetime.now(),test_acc))\n",
    "                test_acc_list.append(test_acc)\n",
    "\n",
    "                #save checkpoint of the model\n",
    "                if ((e+1) % checkpoint_epoch == 0):  \n",
    "                    checkpoint_name = os.path.join(checkpoint_path, dataset_name+'model_fold'+i+'_epoch'+str(e+1)+'.ckpt')\n",
    "                    save_path = saver.save(session, checkpoint_name)  \n",
    "                    print(\"{} Model checkpoint saved at {}\".format(datetime.now(), checkpoint_name))\n",
    "\n",
    "            # find the max test score and the epoch it belongs to        \n",
    "            max_acc.append(max(test_acc_list))\n",
    "            max_epoch = test_acc_list.index(max(test_acc_list))\n",
    "            max_epochs.append(max_epoch) \n",
    "\n",
    "            elapsed_time = time.monotonic() - start_time\n",
    "            text_file.write(\"--- Training time taken: {} ---\\n\".format(time_taken(elapsed_time)))\n",
    "            print(\"--- Training time taken:\",time_taken(elapsed_time),\"---\")\n",
    "            print(\"------------------------\")\n",
    "        \n",
    "        # return the max accuracies of each fold and their respective epochs\n",
    "        print(max_acc)\n",
    "        print(max_epochs)\n",
    "    run += 1\n",
    "\n",
    "elapsed_time_long = time.monotonic() - start_time_long\n",
    "print(\"*** All runs completed ***\")\n",
    "text_file.write(\"Total time taken:\")\n",
    "text_file.write(time_taken(elapsed_time_long))\n",
    "print(\"Total time taken:\",time_taken(elapsed_time_long))\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
